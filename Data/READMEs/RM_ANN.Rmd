---
title: "README for Artificial Neural Network (ANN)"
author: "Group 2"
date: "02 12 2019"
output:
  pdf_document:
    fig_caption: yes
    keep_tex: yes
  html_document:
    df_print: paged
---

**Corresponding R Script:** [ANN](https://github.com/NicSchuler/DSF_NFLDraftPrediction/blob/master/Project_Scripts/ANN.R)

# 1. Introduction

The goal of this project is to make a prediction about the likelilhood of College Football (CFB) players to be drafted into the professional football league (NFL). In order to achieve this, different aspects of a CFB player's college career are being used as features $p_{1}, ..., p_{n}$. These include game statistics such as rush attempts `Rush.Att` or yards ran after a pass `Pass.Yard`. To solve this problem, one of the methods employed is an artificial neural network (ANN). ANNs consist of connected nodes and are not too dissimilar to a human brain in that they derive their "intelligence" from the structure of their "neurons" and the strength of the "synapses" (edges) between these nodes. For this project, an ANN with one input layer with the size of the number of features in the underlying dataset, one hidden layer with ten nodes and an output layer of the size of one was used to solve the CFB player classification problem. The main advantages of ANN include its ability to model complex, non-linear relationships between input and output variables, its ability to infer generalized relationships on data it has never seen before, its robustness towards local minima in the cost function as well as its universality property, meaning it can fit any problem, provided it is sufficiently large.

# 2. ANN Setup and Backpropagation

The feedforward ANN used in this project is set up with three layers: an input layer of the size of the number of features in the data, one hidden layer consisting of ten nodes and an output layer containing a single node. Having one hidden layer was deemed to be sufficient due to the complexity of the underlying problem and the rule of thumb commonly used to determine the size of the hidden layer is to take the mean between the number of nodes in the input layer and the output layer. The output layer was constructed to have one single node because the classification problem at hand only determines between levels of one class (drafted or not). A more generalized approach to ANN setup with respects to the hidden layer would be to go through the process of pruning, meaning starting out with a number of nodes that is rather too large for the underlying problem and then subsequently eliminating acitvations with corresponding weights close to zero.

In order to train the ANN, a process called backpropagation is applied. This refers to the act of feeding labeled data into the network, analyzing the cost (squared difference between the desired and actual output) of the current combination of biases $b_{1},...,b_{i}$, activations $a_{1},...,a_{i}$ and weights $w_{1},...,w_{i}$ and adjusting according to their influence on any given node in the direction of greatest descent in the cost function $\nabla_{a}C$. Every acitvation $a_L$ of layer $L$ is given by the activation function $\sigma'(z_{L})$ with $z_{L}=w_{L}a_{L-1}+b_{L}$. In this project a logistic function was chosen as the activation function:

$$\sigma'(z)=\frac{1}{1+e^{-z}}$$

Subsequently, all changing effects for all neurons in the network are added together and propagated backwards through the network, hence the name backpropagation. This is repeated for all instances of training data resulting in an average change over all training data which is proportional to the negative gradient of the cost function:

$$\delta_L=\nabla_aC\odot\sigma'(z_L)$$

Because every activation is dependent on the bias $b$ and weight $w$ as well as the connected activations from the previous layer, the total change in the cost function is also dependent on the $z$ which sums up these values for the previous layer (the layer index $L$ is given as a superscript):

$$\frac{\partial C_0}{\partial w^{L}}=\frac{\partial z^L}{\partial w^L}\frac{\partial a^L}{\partial z^L}\frac{\partial C_0^L}{\partial a^L}$$

When combined with the sensitivity of the cost function to changes in bias $\frac{\partial C_0}{\partial b^L}$, this yields the combined change to be made (the previous layer is indexed by $k$ and the current layer is indexed by $j$):

$$\frac{\partial C}{\partial w_{jk}^l}=a_{k}^{l-1}\delta_j^l\,\, and \,\,\frac{\partial C}{\partial b_j^l}=\delta_j^l, \,\, with\,\,\delta^l=((w^{l+1})^T\delta^{l+1})\odot\sigma'(z^l)$$

# 3. Data Standardization

Because an ANN posesses the ability to infer generalized relationships on the training data without imposing any limitations on distribution etc., it is not sensitive to differently scaled or centered features and therefore does not change depending on monotonic transformation of the input data. This means that there is no need to standardize the data used, event though it is differently scaled across features (see [RM_DataHandling](https://github.com/NicSchuler/DSF_NFLDraftPrediction/tree/master/Data/READMEs))

# 4. Tuning the model

As previously described, the ANN model could be tuned in terms of number of hidden layers and especially on the number of nodes in the hidden layer(s). Due to time and ressource constraints of this project, this is not done in the ANN script, which leaves room for future improvement. For the case at hand it is simply assumed to be sufficient to have one hidden layer and the number of hidden nodes is determined by the general rule of thumb descriped above. As the topic of ANNs is exceedingly deep, there are a multitude of optimization functions available to optimize ANN performance such as the Beale function, which, again due to project constraints, have been neglected in this project.

# 5. Implementation in R

The application of ANN in R is explained below. All code is included in the [ANN script](https://github.com/NicSchuler/DSF_NFLDraftPrediction/blob/master/Project_Scripts/ANN.R) and step-by-step comments are provided.

## 5.1 Training the ANN Model

For training the data from years 2007 to 2013 of all unsampled and sampled datasets are used respectively. The ANN model is trained using the `nlminb()` function of the `h2o` package which implements quasi-Newton optimization methods. These are used in order to reduce computational power that would be needed for full Newton's methods in order to find the twice differentiable global minimum of a complex non-linear function.

```{r eval=FALSE}
BP_pred <- nlminb(start = ANN_par,
                 objective = ANN_cost, 
                 gradient = ANN_grad,
                 hessian = NULL,
                 L_i_size = L_i_size,
                 L_h_size = L_h_size,
                 L_o_size = L_o_size,
                 x = x, y = y,
                 lambda = 1,
                 control = options)
```

As the `start` parameter, the combined thetas containing only 1s is passed. The `ANN_cost` and `ANN_grad` functions have been previously defined as the cost and gradient function to optimize. The `L_i_size`, `L_h_size` and `L_o_size` arguments define the dimensions of the input, hidden and output layers as described above.

The resulting vetor of the two thetas is then saved to be used for prediction on unlabeled testing data later on:

```{r eval=FALSE}
BP_par <- BP_pred$par
```
 
## 5.2 Variable Importance

Largely due to the universality property of ANNs, variable importance and the construction of hypermodels including preliminary methods for variable selection have not been implemented. This is following the logic that an ANN should be capable of dealing with a mixture of features of differing importance by assigning very small weights to the corresponding nodes. Because it is not clear if the neural network is tuned optimally, it is not possible to give statements about the universality of the specific ANN used in this project. Because there is a possibility that the underlying ANN is oversimplified in respects to the problem at hand, this leaves room for future improvement.

## 5.3 Model Performance

In order to gain an oversight of the model performance, the ANN models are tested on their training (using the years 2007 through 2013) and testing (using the year 2014) fit for all positions and all sampling methods. Both are based on unsampled data, allowing for the model's predictive ability on imbalanced class sizes, as they occur in the underlying business case, to be analyzed. The training and testing data performance can be viewed below (also see the [between model comparison](https://github.com/NicSchuler/DSF_NFLDraftPrediction/blob/master/Data/READMEs/RM_PerformanceMeasurement.pdf)):

```{r, include=FALSE}
library(tidyverse)
load("../PerformanceMeasurement/ANNPerfMeas.Rdata")
ANNPerfMeas1 = ANNPerfMeas %>%
  select(-c(Method, Sampling))
PerfMeas = as.data.frame(t(ANNPerfMeas1))
names(PerfMeas) = c("No Sampling", "Oversampling", "Undersampling", "Rose Both", "Smote")
load("../PerformanceMeasurement/ANNPerfMeas2014.Rdata")
ANNPerfMeas2014_1 = ANNPerfMeas2014 %>%
  select(-c(Method, Sampling))
PerfMeas2014 = as.data.frame(t(ANNPerfMeas2014_1))
names(PerfMeas2014) = c("No Sampling", "Oversampling", "Undersampling", "Rose Both", "Smote")
```
```{r echo = FALSE}
knitr::kable(PerfMeas, caption = "training data performance")
knitr::kable(PerfMeas2014, caption = "testing data performance")
```